{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè† Notebook Overview\n",
    "\n",
    "This notebook presents a comprehensive workflow for data preprocessing, feature engineering, and exploratory analysis for a house price prediction task using the Ames Housing dataset. The main sections and their purposes are outlined below:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. üì• Data Loading and Initial Exploration\n",
    "\n",
    "- **Load and Preview Data:** Training and test datasets are loaded from CSV files. Initial data shapes and previews are displayed to understand the structure and basic statistics.\n",
    "- **Feature Identification:** Numerical and categorical features are identified, and irrelevant columns (like 'Id' and 'SalePrice') are excluded from modeling features.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. üö® Outlier Detection and Removal\n",
    "\n",
    "- **Distribution Analysis:** Kernel density plots are used to visualize feature distributions and identify outliers in columns such as `LotFrontage`, `LotArea`, and `MasVnrArea`.\n",
    "- **Outlier Removal:** Records with extreme values not present in the test set are removed from the training data to improve model generalizability.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. üîó Data Combination for Preprocessing\n",
    "\n",
    "- **Dataset Concatenation:** Training and test datasets are combined to ensure consistent preprocessing and feature engineering across both sets.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. üß© Missing Value Imputation\n",
    "\n",
    "- **Missing Value Summary:** Columns with missing values are identified, and the extent of missingness is summarized for train, test, and combined datasets.\n",
    "- **Imputation Strategies:** \n",
    "    - Contextual imputation (e.g., median `LotFrontage` by `Neighborhood`)\n",
    "    - Filling categorical features with 'NA' or most frequent values\n",
    "    - Filling numerical features with 0 where absence is implied\n",
    "    - Special handling for high-missing-rate columns (e.g., `PoolQC`, `MiscFeature`, `Fence`)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. üõ†Ô∏è Feature Engineering\n",
    "\n",
    "- **Time-Based Features:** New features are created to capture the age of the house, garage, and remodel events, including binary indicators for recent remodels.\n",
    "- **Seasonality Features:** Sale month is encoded as a categorical variable, and monthly average sale prices and counts are mapped to each record.\n",
    "- **Dwelling Type Encoding:** `MSSubClass` is treated as a categorical variable, with additional features for newer dwellings and average prices by subclass.\n",
    "- **Ordinal Encoding:** Quality and condition features are mapped to ordinal scales for improved model interpretability.\n",
    "- **Location Features:** Neighborhoods are mapped to latitude/longitude, and Euclidean distance to city center is calculated. Neighborhoods are also binned by median price.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. üì¶ Final Data Preparation\n",
    "\n",
    "- **Feature Selection:** Top features are selected for modeling based on domain knowledge and exploratory analysis.\n",
    "- **Train/Test Split:** The processed data is split back into training and test sets, ready for modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. üèÜ Kaggle Submission\n",
    "\n",
    "- **Prediction Generation:** Model predictions for the test set are generated using the trained regressor.\n",
    "- **Submission File Creation:** A submission CSV file is created in the required Kaggle format, typically containing `Id` and `SalePrice` columns.\n",
    "- **Submission Workflow:** The submission file can be uploaded to Kaggle for evaluation and leaderboard ranking.\n",
    "\n",
    "Example workflow:\n",
    "1. Generate predictions:  \n",
    "    `preds = xgb_regressor.predict(test_final[top_20_features])`\n",
    "2. Prepare submission DataFrame:  \n",
    "    `submission = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': preds})`\n",
    "3. Save to CSV:  \n",
    "    `submission.to_csv('submission.csv', index=False)`\n",
    "4. Submit on Kaggle:  \n",
    "    Use the Kaggle web interface or CLI to upload `submission.csv`.\n",
    "\n",
    "This final step completes the pipeline, allowing you to benchmark your model on the official Kaggle competition leaderboard.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ Summary\n",
    "\n",
    "This notebook provides a robust pipeline for preparing the Ames Housing dataset for predictive modeling. It covers data cleaning, outlier handling, missing value imputation, feature engineering, and exploratory analysis, with clear documentation and visual support for each step. The resulting processed data is well-structured for downstream machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üì• Data Loading and Initial Exploration\n",
    "\n",
    "This section is essential for understanding the structure, content, and basic statistics of the dataset before any preprocessing or modeling.\n",
    "\n",
    "- üìä **Load and Preview Data:**  \n",
    "    Gain an overview of the training and test datasets, including their shapes and sample records.\n",
    "\n",
    "- üîç **Feature Identification:**  \n",
    "    Identify numerical and categorical features, and exclude irrelevant columns (such as `Id` and `SalePrice`) from modeling.\n",
    "\n",
    "- ‚ö†Ô∏è **Spot Issues:**  \n",
    "    Detect potential problems like missing values or outliers early in the workflow.\n",
    "\n",
    "- üß≠ **Foundation for Workflow:**  \n",
    "    Initial exploration ensures that all subsequent data cleaning and feature engineering steps are based on a clear understanding of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Setup\n",
    "## üìù 1 Create & Prep Your Account\n",
    "1. Go to https://www.kaggle.com and Sign Up / Log In.\n",
    "2. Complete profile (photo, short bio, location) ‚Äì it increases trust when others view your notebooks.\n",
    "3. (Optional but good) Enable 2FA under Settings > Account.\n",
    "\n",
    "## üìú 2 Accept Competition Rules\n",
    "Before you can download data you MUST open the competition page and click: Join Competition / I Understand & Accept. Do this for every new competition (even ‚ÄúGetting Started‚Äù ones) or the API will return 403 errors.\n",
    "\n",
    "## üîë 3 Generate an API Token\n",
    "1. Click your profile avatar (top-right) ‚Üí Settings.\n",
    "2. Scroll to ‚ÄúAPI‚Äù section ‚Üí Create New Token.\n",
    "3. This downloads `kaggle.json` (contains username + key). Keep it private.\n",
    "4. Upload `kaggle.json` to your environment and follow the steps below.\n",
    "5. Move `kaggle.json` to the location `~/.kaggle/kaggle.json`, sometimes `~/.config/kaggle/kaggle.json` (Linux/Mac) or `C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json` (Windows).\n",
    "6. Set file permissions to read-only: `chmod 600 ~/.kaggle/kaggle.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c house-prices-advanced-regression-techniques -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "with zipfile.ZipFile(os.path.join('data','house-prices-advanced-regression-techniques.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training and test datasets from CSV files\n",
    "train_data = pd.read_csv(os.path.join('data', 'train.csv'))\n",
    "test_data = pd.read_csv(os.path.join('data', 'test.csv'))\n",
    "\n",
    "# Print the shape (number of rows and columns) of the training data\n",
    "print(train_data.shape)\n",
    "# Display the first row of the training data for a quick preview\n",
    "display(train_data.head(1))\n",
    "# Uncomment the next line to display detailed info about training data columns and types\n",
    "# display(train_data.info())\n",
    "\n",
    "# Print the shape of the test data\n",
    "print(test_data.shape)\n",
    "# Display the first row of the test data for a quick preview\n",
    "display(test_data.head(1))\n",
    "# Uncomment the next line to display detailed info about test data columns and types\n",
    "# display(test_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all numerical columns from train_data except those of type 'object'\n",
    "previous_num_columns = train_data.select_dtypes(exclude=['object']).columns.values.tolist()\n",
    "\n",
    "# Remove 'Id' and 'SalePrice' since they are not features for modeling\n",
    "previous_num_columns.remove('Id')\n",
    "previous_num_columns.remove('SalePrice')\n",
    "\n",
    "# Print the list of numerical feature columns\n",
    "print(previous_num_columns)\n",
    "\n",
    "# Explanation:\n",
    "# - select_dtypes(exclude=['object']) selects columns with numeric types (int, float, etc.)\n",
    "# - We exclude 'Id' (identifier) and 'SalePrice' (target variable) from the feature list.\n",
    "# - The result is a list of numerical features available for analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üö® Outlier Detection and Removal\n",
    "\n",
    "- **Why Outlier Detection Matters:**  \n",
    "    üö® Outlier detection and removal is essential for building robust predictive models. Outliers can distort statistical analyses, bias model training, and reduce generalizability to unseen data.\n",
    "\n",
    "- **Approach:**  \n",
    "    üîç By identifying and removing extreme values that do not exist in the test set, we ensure the model learns patterns representative of the real-world data distribution.\n",
    "\n",
    "- **Benefits:**  \n",
    "    ‚úÖ Improves accuracy and reliability of predictions  \n",
    "    ‚úÖ Prevents overfitting  \n",
    "    ‚úÖ Enhances interpretability of downstream analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the column to analyze\n",
    "test_column = 'MasVnrArea'\n",
    "\n",
    "# Plot the kernel density estimate (KDE) for the MasVnrArea column in the training data\n",
    "sns.kdeplot(train_data[test_column])\n",
    "# Plot the KDE for the MasVnrArea column in the test data\n",
    "sns.kdeplot(test_data[test_column])\n",
    "\n",
    "# Print the number of records in the training set where MasVnrArea > 1500 (potential outliers)\n",
    "print('train:', train_data[test_column][train_data[test_column] > 1500].shape)\n",
    "# Print the number of records in the test set where MasVnrArea > 1500 (potential outliers)\n",
    "print('test:', test_data[test_column][test_data[test_column] > 1500].shape)\n",
    "\n",
    "# Explanation:\n",
    "# - This code visualizes the distribution of the MasVnrArea feature for both train and test datasets.\n",
    "# - It helps to identify outliers and compare distributions between train and test sets.\n",
    "# - The print statements show how many records have unusually large values (>1500), which can be useful for outlier detection and data cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are some records in the training set with values outside a certain range, which do not exist in the test set. These outliers are removed from the training set.\n",
    "\n",
    "- LotFrontage: 2 records > 200 in train, none in test\n",
    "- LotArea   : 5 records > 70000 in train, none in test\n",
    "- MasVnrArea: 1 record > 1500 in train, none in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the shape of the training data before removing outliers\n",
    "print(train_data.shape)\n",
    "\n",
    "# Remove records from train_data where LotFrontage > 200 (outliers not present in test set)\n",
    "train_data.drop(train_data[train_data[\"LotFrontage\"] > 200].index, inplace=True)\n",
    "\n",
    "# Remove records from train_data where LotArea > 70000 (outliers not present in test set)\n",
    "train_data.drop(train_data[train_data[\"LotArea\"] > 70000].index, inplace=True)\n",
    "\n",
    "# Remove records from train_data where MasVnrArea > 1500 (outliers not present in test set)\n",
    "train_data.drop(train_data[train_data[\"MasVnrArea\"] > 1500].index, inplace=True)\n",
    "\n",
    "# Print the shape of the training data after removing outliers\n",
    "print(train_data.shape)\n",
    "\n",
    "# Store the updated number of records in train_data after outlier removal\n",
    "train_length = train_data.shape[0]\n",
    "\n",
    "# Explanation:\n",
    "# - Outliers in LotFrontage, LotArea, and MasVnrArea are removed from the training set because such extreme values do not exist in the test set.\n",
    "# - This helps prevent the model from learning patterns that are not generalizable to the test data.\n",
    "# - The shape print statements show how many records were removed.\n",
    "# - train_length is updated to reflect the new size of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîó Data Combination for Preprocessing\n",
    "\n",
    "- üîÑ **Why Combine Datasets?**  \n",
    "    Combining training and test datasets before preprocessing ensures that all transformations‚Äîsuch as missing value imputation, encoding, and feature engineering‚Äîare applied consistently. This unified approach prevents discrepancies and data leakage, maintaining data integrity.\n",
    "\n",
    "- üß© **Benefits:**  \n",
    "    - Consistent feature transformations  \n",
    "    - Improved model generalizability  \n",
    "    - Avoids data leakage between train and test sets  \n",
    "    - Simplifies workflow for downstream modeling\n",
    "\n",
    "- üöÄ **Summary:**  \n",
    "    By processing train and test data together, we ensure that the model is trained and evaluated on data that has undergone identical preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test datasets for feature engineering and preprocessing.\n",
    "# - The first line concatenates the training data (up to and including 'SalePrice') and the test data.\n",
    "# - This is useful for applying transformations (like encoding or imputing missing values) consistently across both datasets.\n",
    "conbined_data = pd.concat([train_data.loc[:, : 'SalePrice'], test_data])\n",
    "\n",
    "# Align columns to match the test_data structure.\n",
    "# - This ensures that conbined_data has the same columns as test_data, which is important for making predictions later.\n",
    "conbined_data = conbined_data[test_data.columns]\n",
    "\n",
    "# Display the first row of the combined dataset for a quick preview.\n",
    "display(conbined_data.head(1))\n",
    "\n",
    "# Print the shape (number of rows and columns) of the combined dataset.\n",
    "print(conbined_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üß© Missing Value Imputation\n",
    "\n",
    "Missing value imputation is a crucial step in any data preprocessing workflow.  \n",
    "- üõ°Ô∏è **Ensures Data Integrity:** Handling missing values prevents errors and inconsistencies during analysis and modeling.\n",
    "- üìà **Improves Model Performance:** Proper imputation strategies help models learn from complete data, reducing bias and variance.\n",
    "- üß† **Preserves Information:** Thoughtful imputation (using domain knowledge or feature relationships) retains as much useful information as possible.\n",
    "- üöÄ **Enables Feature Engineering:** Clean, imputed data allows for robust feature creation and transformation in later steps.\n",
    "\n",
    "By addressing missing values systematically, we build a solid foundation for accurate and reliable predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get columns with missing data in the combined dataset\n",
    "has_null_columns = conbined_data.columns[conbined_data.isnull().any()].tolist()\n",
    "\n",
    "# Count missing values for each column in train, test, and combined datasets\n",
    "train_null = train_data[has_null_columns].isnull().sum()\n",
    "test_null = test_data[has_null_columns].isnull().sum()\n",
    "conbined_null = conbined_data[has_null_columns].isnull().sum()\n",
    "\n",
    "print('how many data missed each column of train/test/conbine datas')\n",
    "\n",
    "# Create a DataFrame to summarize missing values for each column across datasets\n",
    "missed_data = pd.DataFrame(\n",
    "    data=[train_null, test_null, conbined_null],\n",
    "    index=['train', 'test', 'conbine'],\n",
    "    columns=has_null_columns\n",
    ")\n",
    "\n",
    "# Display the summary DataFrame\n",
    "missed_data\n",
    "\n",
    "# Explanation:\n",
    "# - has_null_columns: List of columns in conbined_data that have missing values.\n",
    "# - train_null, test_null, conbined_null: Series showing the count of missing values for each column in train, test, and combined datasets.\n",
    "# - missed_data: DataFrame that organizes the missing value counts for easy comparison.\n",
    "# - This helps to quickly identify which columns have missing data and how much is missing in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_missing_conbined_data(column, value):\n",
    "    # Fill missing values in the specified column of conbined_data with the provided value.\n",
    "    # This function locates all rows in conbined_data where the given column is null (missing),\n",
    "    # and sets those entries to the specified value.\n",
    "    # Useful for handling missing data during preprocessing and feature engineering.\n",
    "    conbined_data.loc[conbined_data[column].isnull(),column] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LotFrontage/LotArea**\n",
    "\n",
    "For missing LotFrontage (Linear feet of street connected to property), the average value is usually used for imputation, but it may be related to different Neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the median LotFrontage for each Neighborhood.\n",
    "# This helps visualize how the typical street frontage varies by neighborhood,\n",
    "# which can reveal location-based patterns or differences in lot sizes.\n",
    "conbined_data['LotFrontage'].groupby(conbined_data[\"Neighborhood\"]).median().plot()\n",
    "\n",
    "# Plot the mean LotFrontage for each Neighborhood.\n",
    "# Comparing the mean and median can highlight the effect of outliers or skewed distributions\n",
    "# in LotFrontage within each neighborhood.\n",
    "conbined_data['LotFrontage'].groupby(conbined_data[\"Neighborhood\"]).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the median LotFrontage for each Neighborhood.\n",
    "# This mapping will be used to impute missing LotFrontage values based on the neighborhood.\n",
    "lf_neighbor_map = conbined_data['LotFrontage'].groupby(conbined_data[\"Neighborhood\"]).median()\n",
    "    \n",
    "# Find rows in conbined_data where LotFrontage is missing (NaN).\n",
    "rows = conbined_data['LotFrontage'].isnull()\n",
    "\n",
    "# For each row with missing LotFrontage, fill it with the median LotFrontage of its Neighborhood.\n",
    "# This leverages local neighborhood information for more accurate imputation than using a global mean/median.\n",
    "conbined_data['LotFrontage'][rows] = conbined_data['Neighborhood'][rows].map(lambda neighbor : lf_neighbor_map[neighbor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display rows in conbined_data where 'LotFrontage' is missing (NaN).\n",
    "# This is useful for inspecting which records still have missing values after imputation,\n",
    "# and for verifying that the imputation process worked as expected.\n",
    "# It can also help identify any patterns or issues with the remaining missing data.\n",
    "conbined_data[conbined_data['LotFrontage'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alley**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the shape (number of rows and columns) of the combined dataset,\n",
    "# and the shape of the subset where 'Alley' is missing (NaN).\n",
    "# This helps to quickly check how many records are in the combined data,\n",
    "# and how many records still have missing values for the 'Alley' column.\n",
    "conbined_data.shape, conbined_data[conbined_data['Alley'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data: 2,721 out of 2,919 records are missing (93.2%). Since the missing rate is too high, fill the missing values with \"NA\" (NA = No alley access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'Alley' column of conbined_data with 'NA'.\n",
    "# Explanation:\n",
    "# - The 'Alley' column has a very high rate of missing values (over 93%).\n",
    "# - 'NA' is used to indicate 'No alley access', which is a reasonable assumption given the data.\n",
    "# - This approach prevents issues with downstream analysis or modeling due to missing values.\n",
    "fill_missing_conbined_data('Alley', 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MasVnrType / MasVnrArea**\n",
    "\n",
    "Fill missing values in MasVnrType with the most frequent type, and missing values in MasVnrArea with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values in 'MasVnrType' with 'None'.\n",
    "# Explanation:\n",
    "# - 'MasVnrType' (Masonry veneer type) has many missing values in both train and test sets.\n",
    "# - The most frequent value is 'None', which means no masonry veneer.\n",
    "# - Filling with 'None' is a common approach for categorical features with missing values that indicate absence.\n",
    "conbined_data['MasVnrType'].fillna('None', inplace=True)\n",
    "\n",
    "# Fill missing values in 'MasVnrArea' with 0.\n",
    "# Explanation:\n",
    "# - 'MasVnrArea' (Masonry veneer area) is a numerical feature.\n",
    "# - Missing values likely correspond to houses with no masonry veneer, so 0 is appropriate.\n",
    "# - This ensures the column is fully numeric and avoids issues with downstream modeling.\n",
    "conbined_data['MasVnrArea'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BsmtQual / BsmtCond / BsmtExposure / BsmtFinType1 / BsmtFinType2**\n",
    "- BsmtQual: 37 / 44 / 81\n",
    "- BsmtCond: 37 / 45 / 82\n",
    "- BsmtExposure: 38 / 44 / 82\n",
    "- BsmtFinType1: 37 / 42 / 79\n",
    "- BsmtFinType2: 38 / 42 / 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of basement-related columns for missing value handling and feature engineering.\n",
    "# Explanation:\n",
    "# - These columns include both categorical (quality/type) and numerical (finished area) attributes of the basement.\n",
    "# - 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2' are categorical features describing the basement's quality, condition, exposure, and finish type.\n",
    "# - 'BsmtFinSF1', 'BsmtFinSF2' are numerical features representing the square footage of finished basement areas.\n",
    "# - This list is used to systematically process missing values and transformations for all basement-related features.\n",
    "basement_cols=['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtFinSF1','BsmtFinSF2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing string-type basement columns with 'NA', meaning No Basement\n",
    "# Explanation:\n",
    "# - Iterate over each column in basement_cols.\n",
    "# - For columns that do not contain 'FinSF' in their name (i.e., categorical basement columns),\n",
    "#   fill missing values with 'NA' to indicate 'No Basement'.\n",
    "# - This is a common approach for categorical features where missing values represent absence.\n",
    "for column in basement_cols:\n",
    "    if 'FinSF'not in column:\n",
    "        # NA\tNo Basement\n",
    "        fill_missing_conbined_data(column, 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For numerical BsmtFinSF1 and BsmtFinSF2 columns, fill missing values with 0\n",
    "# Explanation:\n",
    "# - 'BsmtFinSF1' and 'BsmtFinSF2' represent the square footage of finished basement areas.\n",
    "# - Missing values in these columns likely indicate no finished basement area, so filling with 0 is appropriate.\n",
    "# - This ensures the columns remain numeric and avoids issues with downstream analysis or modeling.\n",
    "fill_missing_conbined_data('BsmtFinSF1', 0)\n",
    "fill_missing_conbined_data('BsmtFinSF2', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Electrical**\n",
    "\n",
    "Missing one value, fill with the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the 'Electrical' feature in the combined dataset using a count plot.\n",
    "# Explanation:\n",
    "# - sns.countplot() creates a bar plot showing the frequency of each category in the 'Electrical' column.\n",
    "# - This helps to quickly identify the most common electrical systems and spot any rare or missing categories.\n",
    "# - Useful for understanding categorical feature distributions and guiding missing value imputation or encoding.\n",
    "sns.countplot(conbined_data['Electrical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'Electrical' column of conbined_data with 'SBrkr'.\n",
    "# Explanation:\n",
    "# - The 'Electrical' column has only one missing value in the training set and none in the test set.\n",
    "# - 'SBrkr' (Standard Circuit Breakers) is the most frequent value in both train and test datasets.\n",
    "# - Filling with the most common value is a standard approach for categorical features with very few missing values.\n",
    "# - This ensures consistency and avoids introducing bias from rare categories.\n",
    "fill_missing_conbined_data('Electrical', 'SBrkr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FireplaceQu**\n",
    "\n",
    "For records where Fireplaces is 0, set FireplaceQu to NA, meaning No Fireplace. This accounts for the 1,420 missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For records where Fireplaces is 0, set FireplaceQu to NA, meaning No Fireplace. This accounts for the 1,420 missing values.\n",
    "# Explanation:\n",
    "# - The 'FireplaceQu' column describes the quality of the fireplace.\n",
    "# - Many records have missing values in 'FireplaceQu', which usually means there is no fireplace in the house.\n",
    "# - By filling missing 'FireplaceQu' values with 'NA', we explicitly indicate \"No Fireplace\" for these records.\n",
    "# - This helps downstream feature engineering and modeling by making the absence of a fireplace explicit, rather than leaving it as a missing value.\n",
    "fill_missing_conbined_data('FireplaceQu', 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PoolQC**\n",
    "\n",
    "PoolQC has 2,909 missing values. Is it related to PoolArea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'PoolQC' column of conbined_data with 'NA'.\n",
    "# Explanation:\n",
    "# - 'PoolQC' (Pool Quality) has a very high rate of missing values (almost all records).\n",
    "# - Most houses do not have a pool, so missing values likely indicate \"No Pool\".\n",
    "# - Filling with 'NA' makes this explicit and avoids issues with downstream analysis or modeling.\n",
    "# - This approach is consistent with how other categorical features with high missing rates are handled.\n",
    "fill_missing_conbined_data('PoolQC', 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MiscFeature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'MiscFeature' column of conbined_data with 'NA'.\n",
    "# Explanation:\n",
    "# - 'MiscFeature' describes miscellaneous features not covered by other columns (e.g., shed, tennis court).\n",
    "# - The vast majority of records have missing values in this column, which likely means \"No miscellaneous feature\".\n",
    "# - Filling with 'NA' makes the absence explicit and avoids issues with downstream analysis or modeling.\n",
    "# - This approach is consistent with how other categorical features with high missing rates are handled.\n",
    "fill_missing_conbined_data('MiscFeature', 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'Fence' column of conbined_data with 'NA'.\n",
    "# Explanation:\n",
    "# - The 'Fence' column has a high rate of missing values in both train and test sets (over 80%).\n",
    "# - 'NA' is used to indicate 'No fence', which is a reasonable assumption given the data.\n",
    "# - This approach makes the absence of a fence explicit and avoids issues with downstream analysis or modeling.\n",
    "fill_missing_conbined_data('Fence', 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Garages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of garage-related columns to inspect for missing values and feature engineering.\n",
    "garage_cols=['GarageType','GarageQual','GarageCond','GarageYrBlt','GarageFinish','GarageCars','GarageArea']\n",
    "\n",
    "# Display the first row in conbined_data where 'GarageType' is missing (NaN).\n",
    "# Explanation:\n",
    "# - This helps to inspect the pattern of missing values across all garage-related columns for a record with no garage type.\n",
    "# - Useful for understanding how missingness in 'GarageType' relates to other garage features (e.g., is the whole garage info missing?).\n",
    "# - Can guide imputation strategies (e.g., fill all garage columns with 'NA' or 0 if 'GarageType' is missing).\n",
    "conbined_data[garage_cols][conbined_data['GarageType'].isnull()==True].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing string-type garage columns with 'NA', meaning No Garage\n",
    "# Explanation:\n",
    "# - Iterate over each column in garage_cols.\n",
    "# - For columns that are not 'GarageCars' or 'GarageArea' (i.e., string/categorical garage columns),\n",
    "#   fill missing values with 'NA' to indicate \"No Garage\".\n",
    "# - For 'GarageCars' and 'GarageArea' (numerical columns), fill missing values with 0,\n",
    "#   since missing likely means no garage (so no cars and no area).\n",
    "# - This approach ensures that all garage-related columns have no missing values,\n",
    "#   and the absence of a garage is encoded consistently for both categorical and numerical features.\n",
    "for column in garage_cols:\n",
    "    if column != 'GarageCars' and column != 'GarageArea':\n",
    "        # NA\tNo Basement\n",
    "        fill_missing_conbined_data(column, 'NA')\n",
    "    else:\n",
    "        fill_missing_conbined_data(column, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MSZoning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the 'MSZoning' feature in the combined dataset using a count plot.\n",
    "# Explanation:\n",
    "# - sns.countplot() creates a bar plot showing the frequency of each category in the 'MSZoning' column.\n",
    "# - This helps to quickly identify the most common zoning classifications and spot any rare or missing categories.\n",
    "# - Useful for understanding categorical feature distributions and guiding missing value imputation or encoding.\n",
    "sns.countplot(conbined_data['MSZoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'MSZoning' column of conbined_data with 'RL'.\n",
    "# Explanation:\n",
    "# - 'MSZoning' (zoning classification) has a few missing values in both train and test sets.\n",
    "# - 'RL' (Residential Low Density) is the most common value in the dataset.\n",
    "# - Filling missing values with the most frequent category is a standard approach for categorical features with low missing rates.\n",
    "# - This ensures consistency and avoids introducing bias from rare categories.\n",
    "fill_missing_conbined_data('MSZoning', 'RL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilities**\n",
    "\n",
    "**Definitely ignoring Utilities** : all records are \"AllPub\", except for one \"NoSeWa\" in the train set and 2 NA in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the 'Utilities' feature in the combined dataset using a count plot.\n",
    "# Explanation:\n",
    "# - sns.countplot() creates a bar plot showing the frequency of each category in the 'Utilities' column.\n",
    "# - This helps to quickly identify the most common utility types and spot any rare or missing categories.\n",
    "# - Useful for understanding categorical feature distributions and guiding missing value imputation or encoding.\n",
    "sns.countplot(conbined_data['Utilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'Utilities' column of conbined_data with 'AllPub'.\n",
    "# Explanation:\n",
    "# - The 'Utilities' column has a very low rate of missing values (only 2 in the test set and 0 in the train set).\n",
    "# - 'AllPub' (All public utilities available) is the overwhelmingly dominant value in the dataset.\n",
    "# - Filling missing values with 'AllPub' ensures consistency and avoids introducing bias from rare categories.\n",
    "# - This approach is standard for categorical features with very few missing values and a clear majority value.\n",
    "fill_missing_conbined_data('Utilities', 'AllPub')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exterior1st / Exterior2nd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the 'Exterior1st' feature in the combined dataset using a count plot.\n",
    "# Explanation:\n",
    "# - sns.countplot() creates a bar plot showing the frequency of each category in the 'Exterior1st' column.\n",
    "# - This helps to quickly identify the most common exterior materials and spot any rare or missing categories.\n",
    "# - Useful for understanding categorical feature distributions and guiding missing value imputation or encoding.\n",
    "sns.countplot(conbined_data['Exterior1st'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'Exterior1st' column of conbined_data with 'VinylSd'.\n",
    "# Explanation:\n",
    "# - 'Exterior1st' represents the exterior covering on the house.\n",
    "# - There is 1 missing value in the test set and none in the train set for this column.\n",
    "# - 'VinylSd' (Vinyl Siding) is the most common value in both train and test datasets.\n",
    "# - Filling missing values with the most frequent category is a standard approach for categorical features with low missing rates.\n",
    "# - This ensures consistency and avoids introducing bias from rare categories.\n",
    "fill_missing_conbined_data('Exterior1st', 'VinylSd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the 'Exterior2nd' feature in the combined dataset using a count plot.\n",
    "# Explanation:\n",
    "# - sns.countplot() creates a bar plot showing the frequency of each category in the 'Exterior2nd' column.\n",
    "# - This helps to quickly identify the most common exterior materials and spot any rare or missing categories.\n",
    "# - Useful for understanding categorical feature distributions and guiding missing value imputation or encoding.\n",
    "sns.countplot(conbined_data['Exterior2nd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'Exterior2nd' column of conbined_data with 'VinylSd'.\n",
    "# Explanation:\n",
    "# - 'Exterior2nd' represents the secondary exterior covering on the house.\n",
    "# - There is 1 missing value in the test set and none in the train set for this column.\n",
    "# - 'VinylSd' (Vinyl Siding) is the most common value in both train and test datasets.\n",
    "# - Filling missing values with the most frequent category is a standard approach for categorical features with low missing rates.\n",
    "# - This ensures consistency and avoids introducing bias from rare categories.\n",
    "fill_missing_conbined_data('Exterior2nd', 'VinylSd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BsmtUnfSF / TotalBsmtSF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It can be seen that missing values in TotalBsmtSF and BsmtUnfSF should be filled with 0\n",
    "# Explanation:\n",
    "# - 'TotalBsmtSF' and 'BsmtUnfSF' are basement-related numerical features.\n",
    "# - Missing values in these columns typically indicate that there is no basement area for those records.\n",
    "# - Filling missing values with 0 is appropriate, as it reflects the absence of a basement rather than leaving them as NaN.\n",
    "# - This ensures the columns remain numeric and avoids issues with downstream analysis or modeling.\n",
    "fill_missing_conbined_data('BsmtUnfSF', 0)\n",
    "fill_missing_conbined_data('TotalBsmtSF', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BsmtFullBath / BsmtHalfBath**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values in 'BsmtFullBath' and 'BsmtHalfBath' columns of conbined_data with 0.\n",
    "# Explanation:\n",
    "# - 'BsmtFullBath' and 'BsmtHalfBath' represent the number of full and half bathrooms in the basement.\n",
    "# - Missing values in these columns typically indicate that there is no basement bathroom for those records.\n",
    "# - Filling missing values with 0 is appropriate, as it reflects the absence of basement bathrooms rather than leaving them as NaN.\n",
    "# - This ensures the columns remain numeric and avoids issues with downstream analysis or modeling.\n",
    "fill_missing_conbined_data('BsmtFullBath', 0)\n",
    "fill_missing_conbined_data('BsmtHalfBath', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KitchenQual**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the 'KitchenQual' feature in the combined dataset using a count plot.\n",
    "# Explanation:\n",
    "# - sns.countplot() creates a bar plot showing the frequency of each category in the 'KitchenQual' column.\n",
    "# - This helps to quickly identify the most common kitchen quality ratings and spot any rare or missing categories.\n",
    "# - Useful for understanding categorical feature distributions and guiding missing value imputation or encoding.\n",
    "sns.countplot(conbined_data['KitchenQual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'KitchenQual' column of conbined_data with 'TA'.\n",
    "# Explanation:\n",
    "# - 'KitchenQual' (Kitchen quality) has a small number of missing values in both train and test sets.\n",
    "# - 'TA' (Typical/Average) is the most common value in the dataset for this column.\n",
    "# - Filling missing values with the most frequent category is a standard approach for categorical features with low missing rates.\n",
    "# - This ensures consistency and avoids introducing bias from rare categories.\n",
    "fill_missing_conbined_data('KitchenQual', 'TA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SaleType / Functional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the 'Functional' feature in the combined dataset using a count plot.\n",
    "# Explanation:\n",
    "# - sns.countplot() creates a bar plot showing the frequency of each category in the 'Functional' column.\n",
    "# - This helps to quickly identify the most common functional ratings and spot any rare or missing categories.\n",
    "# - Useful for understanding categorical feature distributions and guiding missing value imputation or encoding.\n",
    "sns.countplot(conbined_data['Functional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'SaleType' column of conbined_data with 'WD'.\n",
    "# Explanation:\n",
    "# - 'SaleType' describes the type of sale (e.g., Warranty Deed, etc.).\n",
    "# - There is only one missing value in the dataset.\n",
    "# - 'WD' (Warranty Deed) is the most common value, so it's used for imputation.\n",
    "fill_missing_conbined_data('SaleType', 'WD')\n",
    "\n",
    "# Fill missing values in the 'Functional' column of conbined_data with 'Typ'.\n",
    "# Explanation:\n",
    "# - 'Functional' describes home functionality (e.g., typical, minor issues, etc.).\n",
    "# - There are only two missing values in the dataset.\n",
    "# - 'Typ' (Typical functionality) is the most frequent value, so it's used for imputation.\n",
    "fill_missing_conbined_data('Functional', 'Typ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completed missing value imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the column names in conbined_data that have missing values (NaN).\n",
    "# - conbined_data.isnull().any() returns a boolean Series indicating which columns have any missing values.\n",
    "# - conbined_data.columns[...] gets the names of those columns.\n",
    "has_null_columns = conbined_data.columns[conbined_data.isnull().any()].tolist()\n",
    "\n",
    "# For each column with missing data, count the number of missing values.\n",
    "# - conbined_data[has_null_columns] selects only the columns with missing values.\n",
    "# - .isnull().sum() counts the number of NaNs in each of those columns.\n",
    "# - This helps to quickly identify which columns still have missing data and how much is missing.\n",
    "conbined_data[has_null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. üõ†Ô∏è Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Why Feature Engineering is Essential\n",
    "\n",
    "Feature engineering transforms raw data into meaningful features that improve model performance and interpretability.  \n",
    "- üöÄ **Boosts Predictive Power:** Carefully crafted features help models capture complex patterns and relationships in the data.\n",
    "- üß† **Incorporates Domain Knowledge:** Custom features reflect real-world insights, making models more relevant and robust.\n",
    "- üîç **Enhances Data Quality:** New features can highlight trends, reduce noise, and address limitations in the original dataset.\n",
    "- üèÜ **Drives Model Success:** Well-engineered features often make the difference between mediocre and outstanding results.\n",
    "\n",
    "This section ensures our house price prediction model leverages the full potential of the Ames Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time Attributes (YearBuilt, GarageYrBlt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select rows from conbined_data where 'GarageYrBlt' is not 'NA', and only keep the 'YearBuilt' and 'GarageYrBlt' columns.\n",
    "# Explanation:\n",
    "# - conbined_data: The combined dataset containing both train and test data after preprocessing.\n",
    "# - 'GarageYrBlt' column contains the year the garage was built, but some entries are 'NA' (missing).\n",
    "# - This line filters out rows where 'GarageYrBlt' is 'NA', so only rows with valid garage build years are kept.\n",
    "# - The resulting DataFrame, built_year_data, will be used for further analysis or modeling, such as checking correlation or fitting a regression model.\n",
    "built_year_data = conbined_data[['YearBuilt', 'GarageYrBlt']][conbined_data['GarageYrBlt'] != 'NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the 'GarageYrBlt' column in built_year_data from string/object to integer type.\n",
    "# Explanation:\n",
    "# - Some values in 'GarageYrBlt' may be stored as strings (e.g., '1998') due to earlier imputation or merging.\n",
    "# - Using map(lambda g : int(g)) ensures all values are integers, which is necessary for correlation analysis.\n",
    "built_year_data['GarageYrBlt'] = built_year_data['GarageYrBlt'].map(lambda g : int(g))\n",
    "\n",
    "# Calculate the Pearson correlation coefficient between 'GarageYrBlt' and 'YearBuilt'.\n",
    "# Explanation:\n",
    "# - .corr() computes the linear correlation between the two columns.\n",
    "# - A high correlation indicates that the year the garage was built is closely related to the year the house was built.\n",
    "# - This information is useful for feature engineering and understanding data relationships.\n",
    "built_year_data['GarageYrBlt'].corr(built_year_data['YearBuilt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YearBuilt and GarageYrBlt are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the first row of built_year_data for inspection.\n",
    "# Explanation:\n",
    "# - built_year_data contains 'YearBuilt' and 'GarageYrBlt' columns for rows where 'GarageYrBlt' is not 'NA'.\n",
    "# - Using .head(1) shows the first record, which is useful for verifying the structure and values after filtering and before further analysis.\n",
    "# - This helps ensure that the data preparation steps for time attributes are correct before proceeding with feature engineering or modeling.\n",
    "built_year_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import LinearRegression from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a LinearRegression model instance\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Extract GarageYrBlt and YearBuilt columns from built_year_data\n",
    "garage_year = built_year_data.loc[:,'GarageYrBlt'].values\n",
    "built_year = built_year_data.loc[:,'YearBuilt'].values\n",
    "\n",
    "# Get the number of samples\n",
    "length = garage_year.shape[0]\n",
    "\n",
    "# Reshape arrays to be 2D, as required by scikit-learn\n",
    "garage_year = garage_year.reshape(length, 1)\n",
    "built_year = built_year.reshape(length, 1)\n",
    "\n",
    "# Fit the regression model: predict GarageYrBlt from YearBuilt\n",
    "regr.fit(built_year, garage_year)\n",
    "\n",
    "# Plot the data points (YearBuilt vs GarageYrBlt) as blue dots\n",
    "plt.scatter(built_year, garage_year,  color='blue')\n",
    "\n",
    "# Plot the regression line (predicted GarageYrBlt) in red\n",
    "plt.plot(built_year, regr.predict(built_year), color='red',\n",
    "         linewidth=3)\n",
    "\n",
    "# Explanation:\n",
    "# - This code fits a linear regression model to predict the year the garage was built (GarageYrBlt)\n",
    "#   based on the year the house was built (YearBuilt), using only rows where GarageYrBlt is not 'NA'.\n",
    "# - The scatter plot visualizes the actual data points, and the red line shows the fitted regression.\n",
    "# - This model will be used to impute missing GarageYrBlt values later, based on YearBuilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill NA values in GarageYrBlt using the regression model\n",
    "# Explanation:\n",
    "# - This code replaces 'NA' values in the 'GarageYrBlt' column of conbined_data with predicted values from a linear regression model.\n",
    "# - For each row, if 'GarageYrBlt' is 'NA', it uses the trained regression model (regr) to predict the garage build year based on 'YearBuilt'.\n",
    "# - The prediction is made by passing the 'YearBuilt' value as input to regr.predict, and the result is converted to an integer.\n",
    "# - If 'GarageYrBlt' is not 'NA', it simply converts the existing value to an integer.\n",
    "# - The operation is applied row-wise (axis=1) using DataFrame.apply.\n",
    "conbined_data['GarageYrBlt'] = conbined_data.apply(\n",
    "    lambda row: int(regr.predict(np.array([[row['YearBuilt']]]))[0]) if row['GarageYrBlt'] == 'NA' else int(row['GarageYrBlt']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the first 5 values of the 'GarageYrBlt' column in conbined_data.\n",
    "# Explanation:\n",
    "# - conbined_data['GarageYrBlt'] contains the year the garage was built for each record in the combined dataset.\n",
    "# - .head() returns the first 5 entries, which is useful for quickly inspecting the results of previous feature engineering steps.\n",
    "# - This helps verify that missing values have been imputed and that the column is correctly formatted as integers.\n",
    "conbined_data['GarageYrBlt'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YearBuilt: Year the house was built  \n",
    "YearRemodAdd: Year the house was remodeled  \n",
    "Used to determine whether the house has been renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many years has remoded from built\n",
    "# Calculate the number of years between when the house was built and when it was remodeled.\n",
    "# This feature (RemodYears) helps capture how long after construction a renovation occurred.\n",
    "conbined_data['RemodYears'] = conbined_data['YearRemodAdd'] - conbined_data['YearBuilt']\n",
    "\n",
    "# Did a remodeling happened from built?\n",
    "# Create a binary feature (HasRemodeled) indicating whether the house has ever been remodeled.\n",
    "# If YearRemodAdd is different from YearBuilt, set to 1 (remodeled), else 0 (not remodeled).\n",
    "conbined_data[\"HasRemodeled\"] = (conbined_data[\"YearRemodAdd\"] != conbined_data[\"YearBuilt\"]) * 1\n",
    "\n",
    "# Did a remodeling happen in the year the house was sold?\n",
    "# Create a binary feature (HasRecentRemodel) indicating if the remodeling occurred in the year of sale.\n",
    "# If YearRemodAdd equals YrSold, set to 1 (recent remodel), else 0.\n",
    "conbined_data[\"HasRecentRemodel\"] = (conbined_data[\"YearRemodAdd\"] == conbined_data[\"YrSold\"]) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the number of years between when the garage was built and when the house was built.\n",
    "# Explanation:\n",
    "# - conbined_data['GarageYrBlt']: The year the garage was built (after missing value imputation).\n",
    "# - conbined_data['YearBuilt']: The year the house was built.\n",
    "# - Subtracting YearBuilt from GarageYrBlt gives the number of years after the house was built that the garage was added.\n",
    "# - If the garage was built in the same year as the house, the result is 0.\n",
    "# - This feature (GarageBltYears) helps capture whether the garage is original or added later, which may affect house value.\n",
    "conbined_data['GarageBltYears'] = conbined_data['GarageYrBlt'] - conbined_data['YearBuilt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the first 5 values of the 'GarageBltYears' column in conbined_data.\n",
    "# Explanation:\n",
    "# - conbined_data['GarageBltYears'] contains the difference between the year the garage was built and the year the house was built.\n",
    "# - .head() returns the first 5 entries, which is useful for quickly inspecting the results of previous feature engineering steps.\n",
    "# - This helps verify that the feature was calculated correctly and provides insight into how often garages are built at the same time as the house or added later.\n",
    "conbined_data['GarageBltYears'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many years has build now?\n",
    "# Explanation:\n",
    "# - This block creates three new features in conbined_data, each representing how many years ago a key event happened (relative to 2017).\n",
    "# - 'Now_YearBuilt': Number of years since the house was built (2017 - YearBuilt).\n",
    "# - 'Now_YearRemodAdd': Number of years since the house was remodeled (2017 - YearRemodAdd).\n",
    "# - 'Now_GarageYrBlt': Number of years since the garage was built (2017 - GarageYrBlt).\n",
    "# - These features help capture the \"age\" of the house, its last remodel, and the garage, which are often important predictors for house price.\n",
    "conbined_data['Now_YearBuilt'] = 2017 - conbined_data['YearBuilt']\n",
    "conbined_data['Now_YearRemodAdd'] = 2017 - conbined_data['YearRemodAdd']\n",
    "conbined_data['Now_GarageYrBlt'] = 2017 - conbined_data['GarageYrBlt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the first 3 rows of the engineered time-based features:\n",
    "# - 'Now_YearBuilt': Number of years since the house was built (2017 - YearBuilt)\n",
    "# - 'Now_YearRemodAdd': Number of years since the house was remodeled (2017 - YearRemodAdd)\n",
    "# - 'Now_GarageYrBlt': Number of years since the garage was built (2017 - GarageYrBlt)\n",
    "# These features help capture the \"age\" of the house, its last remodel, and the garage,\n",
    "# which are important predictors for house price.\n",
    "conbined_data[['Now_YearBuilt','Now_YearRemodAdd','Now_GarageYrBlt']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a peak season for house sales by month\n",
    "- The month in which the house is sold is a numeric type, so it is converted to a string type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the average SalePrice for each month the house was sold.\n",
    "# Explanation:\n",
    "# - train_data['SalePrice']: The target variable (house price) from the training set.\n",
    "# - groupby(train_data['MoSold']): Groups the SalePrice values by the month sold (MoSold).\n",
    "# - mean(): Calculates the mean SalePrice for each month.\n",
    "# - plot(): Plots the resulting monthly average SalePrice as a line plot.\n",
    "# - This visualization helps identify seasonal trends in house prices, showing which months tend to have higher or lower average sale prices.\n",
    "train_data['SalePrice'].groupby(train_data['MoSold']).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of house sales by month using a count plot.\n",
    "# Explanation:\n",
    "# - sns.countplot() creates a bar plot showing the number of houses sold in each month ('MoSold').\n",
    "# - conbined_data['MoSold'] contains the month (as a string) when each house was sold.\n",
    "# - This plot helps identify seasonal patterns in house sales, such as peak and low sales months.\n",
    "# - Useful for feature engineering and understanding how sale timing may affect price or volume.\n",
    "sns.countplot(conbined_data['MoSold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of houses sold each month and the average price are basically inversely proportional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the average SalePrice for each month sold in the training data.\n",
    "# Explanation:\n",
    "# - train_data['SalePrice']: The target variable (house price) from the training set.\n",
    "# - groupby(train_data['MoSold']): Groups the SalePrice values by the month sold (MoSold).\n",
    "# - mean(): Calculates the mean SalePrice for each month.\n",
    "# - to_dict(): Converts the result to a dictionary mapping month to average price.\n",
    "sale_price_month = train_data['SalePrice'].groupby(train_data['MoSold']).mean().to_dict()\n",
    "\n",
    "# Map the average sale price for each month to the combined dataset.\n",
    "# Explanation:\n",
    "# - conbined_data[\"MoSold\"]: The month each house was sold (as a string).\n",
    "# - .replace(sale_price_month): Replaces each month value with its corresponding average SalePrice from the dictionary.\n",
    "# - The new feature \"MonthSaledMeanPrice\" represents the average sale price for the month in which each house was sold.\n",
    "conbined_data[\"MonthSaledMeanPrice\"] = conbined_data[\"MoSold\"].replace(sale_price_month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the number of sales per month.\n",
    "# Explanation:\n",
    "# - sale_month is initialized as a dictionary with keys as string representations of months (\"1\" to \"12\"), all set to 0.\n",
    "# - The for loop iterates over all values in the 'MoSold' column of conbined_data.\n",
    "# - For each month value m, it is converted to a string and used as a key to increment the corresponding count in sale_month.\n",
    "# - This results in sale_month containing the total number of sales for each month, which can be used for feature engineering or analysis.\n",
    "sale_month = {\"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0, \"6\": 0, \"7\": 0, \"8\": 0, \"9\": 0, \"10\": 0, \"11\": 0, \"12\": 0}\n",
    "for m in conbined_data['MoSold'].values:\n",
    "    sale_month[str(m)] = sale_month[str(m)] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert month from numeric type to string type\n",
    "# Explanation:\n",
    "# - conbined_data['MoSold'] contains the month each house was sold, currently as an integer (e.g., 1, 2, ..., 12).\n",
    "# - The .map(lambda m: str(m)) function converts each integer month value to a string (e.g., '1', '2', ..., '12').\n",
    "# - This is useful for categorical encoding, plotting, or feature engineering where months are treated as categories rather than numbers.\n",
    "conbined_data['MoSold'] = conbined_data['MoSold'].map(lambda m: str(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of sales in each month\n",
    "# Explanation:\n",
    "# - sale_month is a dictionary where keys are string representations of months (\"1\" to \"12\") and values are the number of sales in each month.\n",
    "# - conbined_data[\"MoSold\"] contains the month each house was sold, as a string.\n",
    "# - .replace(sale_month) maps each month in \"MoSold\" to its corresponding count from sale_month.\n",
    "# - The result is a new feature \"MonthSaledCount\" in conbined_data, representing the total number of sales for the month in which each house was sold.\n",
    "conbined_data[\"MonthSaledCount\"] = conbined_data[\"MoSold\"].replace(sale_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The numerical value of MSSubClass only represents the type of dwelling, so it is encoded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the average SalePrice for each MSSubClass category in the training data.\n",
    "# Explanation:\n",
    "# - train_data['SalePrice']: The target variable (house price) from the training set.\n",
    "# - groupby(train_data['MSSubClass']): Groups the SalePrice values by the MSSubClass (building class/type).\n",
    "# - mean(): Calculates the mean SalePrice for each MSSubClass category.\n",
    "# - plot(): Plots the resulting average SalePrice for each MSSubClass as a line plot.\n",
    "# - This visualization helps to identify how house prices vary by dwelling type, showing which building classes tend to have higher or lower average sale prices.\n",
    "train_data['SalePrice'].groupby(train_data['MSSubClass']).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the 'MSSubClass' feature in the combined dataset using a count plot.\n",
    "# Explanation:\n",
    "# - sns.countplot() creates a bar plot showing the frequency of each category in the 'MSSubClass' column.\n",
    "# - conbined_data['MSSubClass'] contains the building class/type for each house.\n",
    "# - This plot helps to quickly identify which dwelling types are most common and spot any rare categories.\n",
    "# - Useful for understanding categorical feature distributions and guiding feature engineering or encoding.\n",
    "sns.countplot(conbined_data['MSSubClass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that both price and sales volume are related to MSSubClass. According to the data description, this is because of whether the house is NEWER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a new feature \"NewerDwelling\" to conbined_data based on MSSubClass.\n",
    "# Explanation:\n",
    "# - MSSubClass is a categorical variable representing the type of dwelling.\n",
    "# - Certain MSSubClass values (20, 60, 120) are considered \"newer\" types of dwellings and are mapped to 1.\n",
    "# - All other MSSubClass values are mapped to 0, indicating older dwelling types.\n",
    "# - The mapping is applied using .replace(), which substitutes the specified values in the MSSubClass column.\n",
    "# - This feature helps distinguish between newer and older house types, which can be useful for modeling house prices.\n",
    "conbined_data[\"NewerDwelling\"] = conbined_data[\"MSSubClass\"].replace(\n",
    "    {20: 1, 30: 0, 40: 0, 45: 0,50: 0, 60: 1, 70: 0, 75: 0, 80: 0, 85: 0,\n",
    "     90: 0, 120: 1, 150: 0, 160: 0, 180: 0, 190: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the average price for each category\n",
    "# Explanation:\n",
    "# - train_data['SalePrice']: The target variable (house price) from the training set.\n",
    "# - groupby(train_data['MSSubClass']): Groups the SalePrice values by the MSSubClass (building class/type).\n",
    "# - mean(): Calculates the mean SalePrice for each MSSubClass category.\n",
    "# - to_dict(): Converts the result to a dictionary mapping MSSubClass to average price.\n",
    "sale_price_mssc = train_data['SalePrice'].groupby(train_data['MSSubClass']).mean().to_dict()\n",
    "\n",
    "# Average price sold for each category\n",
    "# Explanation:\n",
    "# - conbined_data[\"MSSubClass\"]: The MSSubClass value for each record in the combined dataset.\n",
    "# - .replace(sale_price_mssc): Replaces each MSSubClass value with its corresponding average SalePrice from the dictionary.\n",
    "# - The new feature \"MSSubClassMeanPrice\" represents the average sale price for the MSSubClass of each house.\n",
    "conbined_data[\"MSSubClassMeanPrice\"] = conbined_data[\"MSSubClass\"].replace(sale_price_mssc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a mapping dictionary to encode MSSubClass values as strings.\n",
    "# Explanation:\n",
    "# - MSSubClass is a categorical feature represented by integers (e.g., 20, 30, 40, ...).\n",
    "# - For modeling and feature engineering, it's common to convert these to string codes (e.g., 'SC20', 'SC30', ...).\n",
    "# - This helps treat MSSubClass as a categorical variable rather than a numeric one, preventing unintended ordinal relationships.\n",
    "mssubclass_dict = {\n",
    "    20: 'SC20',\n",
    "    30: 'SC30',\n",
    "    40: 'SC40',\n",
    "    45: 'SC45',\n",
    "    50: 'SC50',\n",
    "    60: 'SC60',\n",
    "    70: 'SC70',\n",
    "    75: 'SC75',\n",
    "    80: 'SC80',\n",
    "    85: 'SC85',\n",
    "    90: 'SC90',\n",
    "    120: 'SC120',\n",
    "    150: 'SC150',\n",
    "    160: 'SC160',\n",
    "    180: 'SC180',\n",
    "    190: 'SC190',\n",
    "}\n",
    "\n",
    "# Replace the integer MSSubClass values in conbined_data with their corresponding string codes.\n",
    "# Explanation:\n",
    "# - conbined_data['MSSubClass'] contains the original integer values.\n",
    "# - .replace(mssubclass_dict) substitutes each integer with its mapped string code.\n",
    "# - This transformation is useful for downstream encoding (e.g., one-hot encoding) and improves model interpretability.\n",
    "conbined_data['MSSubClass'] = conbined_data['MSSubClass'].replace(mssubclass_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For attributes that have ordinal or quality levels, encode them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mapping dictionary for ordinal and quality features.\n",
    "# Explanation:\n",
    "# - good_level_map defines how to convert categorical values to ordinal numeric values for various features.\n",
    "# - Each key is a column name, and its value is a dictionary mapping category to integer.\n",
    "# - This is useful for features where the order or quality level matters (e.g., 'ExterQual', 'BsmtQual', etc.).\n",
    "good_level_map = {'Street': {'Grvl': 0, 'Pave': 1},\n",
    "    'Alley': {'NA':0, 'Grvl': 1, 'Pave': 2},\n",
    "    'Utilities': {'AllPub':3, 'NoSeWa': 1, 'NoSewr': 2, 'ELO': 0},\n",
    "    'ExterQual': {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1,'Po': 0},\n",
    "    'ExterCond': {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1,'Po': 0},\n",
    "    'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1,'NA': 0},\n",
    "    'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2,'Po': 1,'NA': 0},\n",
    "    'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2,'Po': 1,'NA': 0},\n",
    "    'BsmtFinType1': {'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6},\n",
    "    'BsmtFinType2': {'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6},\n",
    "    'HeatingQC': {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1,'Po': 0},\n",
    "    'CentralAir': {'N':0, 'Y':1},\n",
    "    'KitchenQual': {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0},\n",
    "    'Functional': {'Typ':0,'Min1':1,'Min2':1,'Mod':2,'Maj1':3,'Maj2':4,'Sev':5,'Sal': 6},\n",
    "    'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'PoolQC': {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'NA': 0},\n",
    "    'Fence': {'GdPrv': 2, 'GdWo': 2, 'MnPrv': 1, 'MnWw': 1, 'NA': 0}\n",
    "    }\n",
    "\n",
    "print(good_level_map.keys())  # Print the keys (column names) that will be mapped.\n",
    "\n",
    "# Select columns from conbined_data that are in good_level_map and replace their values using the mapping.\n",
    "# Explanation:\n",
    "# - conbined_data[good_level_map.keys()] selects only the relevant columns.\n",
    "# - .replace(good_level_map) applies the mapping to convert categorical values to ordinal integers.\n",
    "good_level_data = conbined_data[good_level_map.keys()].replace(good_level_map)\n",
    "\n",
    "# Rename columns by adding an underscore at the end.\n",
    "# Explanation:\n",
    "# - This helps distinguish the newly encoded columns from the originals.\n",
    "# - .map(lambda m : m + '_') appends '_' to each column name.\n",
    "good_level_data.columns = good_level_data.columns.map(lambda m : m + '_')\n",
    "\n",
    "# Assign the encoded columns back to conbined_data.\n",
    "# Explanation:\n",
    "# - This adds the new ordinal columns to the main dataframe, preserving the original columns.\n",
    "conbined_data[good_level_data.columns] = good_level_data[good_level_data.columns]\n",
    "\n",
    "print (conbined_data.shape)  # Print the shape of the updated dataframe to confirm changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood Â±ûÊÄßË°®Á§∫ÁöÑÊòØÈôÑËøëÁöÑÂú∞ÂêçÔºåÂèØÂ∞ÜÂÖ∂ËΩ¨‰∏∫ÁªèÁ∫¨Â∫¶„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map latitude values to each Neighborhood in conbined_data.\n",
    "# Explanation:\n",
    "# - conbined_data[\"Neighborhood\"] contains the neighborhood name for each house.\n",
    "# - .replace({...}) maps each neighborhood to its corresponding latitude value.\n",
    "# - This creates a new column \"latitude\" with the latitude for each house based on its neighborhood.\n",
    "conbined_data[\"latitude\"] = conbined_data.Neighborhood.replace(\n",
    "                                               {'Blmngtn' : 42.062806,\n",
    "                                                'Blueste' : 42.009408,\n",
    "                                                'BrDale' : 42.052500,\n",
    "                                                'BrkSide': 42.033590,\n",
    "                                                'ClearCr': 42.025425,\n",
    "                                                'CollgCr': 42.021051,\n",
    "                                                'Crawfor': 42.025949,\n",
    "                                                'Edwards': 42.022800,\n",
    "                                                'Gilbert': 42.027885,\n",
    "                                                'GrnHill': 42.000854,\n",
    "                                                'IDOTRR' : 42.019208,\n",
    "                                                'Landmrk': 42.044777,\n",
    "                                                'MeadowV': 41.991866,\n",
    "                                                'Mitchel': 42.031307,\n",
    "                                                'NAmes'  : 42.042966,\n",
    "                                                'NoRidge': 42.050307,\n",
    "                                                'NPkVill': 42.050207,\n",
    "                                                'NridgHt': 42.060356,\n",
    "                                                'NWAmes' : 42.051321,\n",
    "                                                'OldTown': 42.028863,\n",
    "                                                'SWISU'  : 42.017578,\n",
    "                                                'Sawyer' : 42.033611,\n",
    "                                                'SawyerW': 42.035540,\n",
    "                                                'Somerst': 42.052191,\n",
    "                                                'StoneBr': 42.060752,\n",
    "                                                'Timber' : 41.998132,\n",
    "                                                'Veenker': 42.040106})\n",
    "\n",
    "# Map longitude values to each Neighborhood in conbined_data.\n",
    "# Explanation:\n",
    "# - Similar to latitude, this maps each neighborhood to its longitude value.\n",
    "# - The new column \"longitude\" contains the longitude for each house based on its neighborhood.\n",
    "conbined_data[\"longitude\"] = conbined_data.Neighborhood.replace(\n",
    "                                               {'Blmngtn' : -93.639963,\n",
    "                                                'Blueste' : -93.645543,\n",
    "                                                'BrDale' : -93.628821,\n",
    "                                                'BrkSide': -93.627552,\n",
    "                                                'ClearCr': -93.675741,\n",
    "                                                'CollgCr': -93.685643,\n",
    "                                                'Crawfor': -93.620215,\n",
    "                                                'Edwards': -93.663040,\n",
    "                                                'Gilbert': -93.615692,\n",
    "                                                'GrnHill': -93.643377,\n",
    "                                                'IDOTRR' : -93.623401,\n",
    "                                                'Landmrk': -93.646239,\n",
    "                                                'MeadowV': -93.602441,\n",
    "                                                'Mitchel': -93.626967,\n",
    "                                                'NAmes'  : -93.613556,\n",
    "                                                'NoRidge': -93.656045,\n",
    "                                                'NPkVill': -93.625827,\n",
    "                                                'NridgHt': -93.657107,\n",
    "                                                'NWAmes' : -93.633798,\n",
    "                                                'OldTown': -93.615497,\n",
    "                                                'SWISU'  : -93.651283,\n",
    "                                                'Sawyer' : -93.669348,\n",
    "                                                'SawyerW': -93.685131,\n",
    "                                                'Somerst': -93.643479,\n",
    "                                                'StoneBr': -93.628955,\n",
    "                                                'Timber' : -93.648335,\n",
    "                                                'Veenker': -93.657032})\n",
    "\n",
    "# Calculate the Euclidean distance from each house to the center of Ames city.\n",
    "# Explanation:\n",
    "# - Ames city center coordinates: longitude -93.63191310000002, latitude 42.0307812.\n",
    "# - For each house, subtract the city center coordinates from its longitude and latitude, square the differences, sum them, and take the square root.\n",
    "# - This gives the straight-line distance from the house to the city center, which can be useful for modeling location effects.\n",
    "conbined_data[\"NeighborDistance\"] = np.sqrt(np.power((conbined_data[\"longitude\"] - (-93.63191310000002)),2) + \\\n",
    "                                    np.power((conbined_data[\"latitude\"] - 42.0307812),2))\n",
    "\n",
    "# Display the first 5 values of the calculated NeighborDistance for inspection.\n",
    "display(conbined_data[\"NeighborDistance\"].head())\n",
    "\n",
    "# Remove the temporary longitude and latitude columns from conbined_data.\n",
    "# Explanation:\n",
    "# - These columns were only needed for distance calculation and are not required for further analysis.\n",
    "conbined_data.drop(['longitude', 'latitude'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bin by neighborhood (a little arbitrarily).\n",
    "# Explanation:\n",
    "# - This code creates a mapping from each neighborhood to its median SalePrice in the training data.\n",
    "# - train_data[\"SalePrice\"].groupby(train_data[\"Neighborhood\"]) groups SalePrice values by Neighborhood.\n",
    "# - .median() computes the median SalePrice for each neighborhood.\n",
    "# - .sort_values() sorts the neighborhoods by their median SalePrice in ascending order.\n",
    "# - .to_dict() converts the sorted Series to a dictionary, where keys are neighborhood names and values are the corresponding median prices.\n",
    "# - neighbor_price_map can be used for feature engineering, such as assigning a price bin or encoding neighborhoods by their typical price level.\n",
    "neighbor_price_map = train_data[\"SalePrice\"].groupby(train_data[\"Neighborhood\"]).median().sort_values().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neighbor_bin_map: Dictionary mapping each Neighborhood to a price bin (0-4).\n",
    "# Explanation:\n",
    "# - Each key is a neighborhood name, and its value is an integer bin representing the typical price level.\n",
    "# - The bins are assigned based on the median SalePrice for each neighborhood (see neighbor_price_map).\n",
    "# - Lower bin numbers correspond to lower-priced neighborhoods, higher bins to higher-priced ones.\n",
    "neighbor_bin_map = {\n",
    "    \"MeadowV\" : 0,  #  88000\n",
    "    \"IDOTRR\" : 1,   # 103000\n",
    "    \"BrDale\" : 1,   # 106000\n",
    "    \"OldTown\" : 1,  # 119000\n",
    "    \"Edwards\" : 1,  # 119500\n",
    "    \"BrkSide\" : 1,  # 124300\n",
    "    \"Sawyer\" : 1,   # 135000\n",
    "    \"Blueste\" : 1,  # 137500\n",
    "    \"SWISU\" : 2,    # 139500\n",
    "    \"NAmes\" : 2,    # 140000\n",
    "    \"NPkVill\" : 2,  # 146000\n",
    "    \"Mitchel\" : 2,  # 153500\n",
    "    \"SawyerW\" : 2,  # 179900\n",
    "    \"Gilbert\" : 2,  # 181000\n",
    "    \"NWAmes\" : 2,   # 182900\n",
    "    \"Blmngtn\" : 2,  # 191000\n",
    "    \"CollgCr\" : 2,  # 197200\n",
    "    \"ClearCr\" : 3,  # 200250\n",
    "    \"Crawfor\" : 3,  # 200624\n",
    "    \"Veenker\" : 3,  # 218000\n",
    "    \"Somerst\" : 3,  # 225500\n",
    "    \"Timber\" : 3,   # 228475\n",
    "    \"StoneBr\" : 4,  # 278000\n",
    "    \"NoRidge\" : 4,  # 290000\n",
    "    \"NridgHt\" : 4,  # 315000\n",
    "}\n",
    "\n",
    "# Map the median SalePrice for each Neighborhood to the \"NeighborPrice\" column in conbined_data.\n",
    "# Explanation:\n",
    "# - neighbor_price_map: Dictionary mapping neighborhood names to their median SalePrice (from train_data).\n",
    "# - conbined_data[\"Neighborhood\"].map(neighbor_price_map) assigns the median price to each record based on its neighborhood.\n",
    "# - This feature captures the typical price level for each neighborhood, useful for modeling.\n",
    "conbined_data[\"NeighborPrice\"] = conbined_data[\"Neighborhood\"].map(neighbor_price_map)\n",
    "\n",
    "# Map the price bin for each Neighborhood to the \"NeighborBin\" column in conbined_data.\n",
    "# Explanation:\n",
    "# - neighbor_bin_map: Dictionary mapping neighborhood names to price bins (0-4).\n",
    "# - conbined_data[\"Neighborhood\"].map(neighbor_bin_map) assigns the bin to each record based on its neighborhood.\n",
    "# - This feature provides a categorical representation of neighborhood price levels for downstream analysis or modeling.\n",
    "conbined_data[\"NeighborBin\"] = conbined_data[\"Neighborhood\"].map(neighbor_bin_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the combined dataset (conbined_data).\n",
    "# Explanation:\n",
    "# - conbined_data contains both the training and test data after preprocessing and feature engineering.\n",
    "# - .head() returns the first 5 records, which is useful for quickly inspecting the structure and values of the dataset.\n",
    "# - This helps verify that all previous steps (missing value imputation, feature engineering, encoding, etc.) have been applied correctly.\n",
    "conbined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: [juliencs : A study on Regression applied to the Ames dataset\n",
    "](https://www.kaggle.com/juliencs/house-prices-advanced-regression-techniques/a-study-on-regression-applied-to-the-ames-dataset)\n",
    "- Create some boolean features\n",
    "- Simplifications of existing features - Ref\n",
    "- Combinations of existing features - Ref\n",
    "- Polynomials on the top 10 existing features - Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all columns in conbined_data with string (object) dtype.\n",
    "# - select_dtypes(include=['object']) selects columns where the data type is 'object', typically used for categorical or text data.\n",
    "# - .columns.values returns the column names as a numpy array.\n",
    "str_columns = conbined_data.select_dtypes(include=['object']).columns.values\n",
    "\n",
    "# Get all columns in conbined_data with numeric dtype (excluding 'object').\n",
    "# - select_dtypes(exclude=['object']) selects columns with numeric types (int, float, etc.).\n",
    "# - .columns.values returns the column names as a numpy array.\n",
    "num_columns = conbined_data.select_dtypes(exclude=['object']).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# str_columns contains all columns in conbined_data with string (object) dtype.\n",
    "# - select_dtypes(include=['object']) selects columns where the data type is 'object', typically used for categorical or text data.\n",
    "# - .columns.values returns the column names as a numpy array.\n",
    "# - This is useful for identifying which columns need encoding or special handling during feature engineering.\n",
    "str_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Create some boolean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test str column\n",
    "# This block analyzes the distribution of values in a string (categorical) column of conbined_data.\n",
    "column = \"SaleCondition\"  # The column to analyze (can be changed to any string column)\n",
    "count_duct = {}  # Initialize an empty dictionary to store counts for each unique value\n",
    "\n",
    "# Iterate over all unique values in the column and initialize their count to 0\n",
    "for key in set(conbined_data[column]):\n",
    "    count_duct[key] = 0\n",
    "    \n",
    "# Count the occurrences of each value in the column\n",
    "for m in conbined_data[column].values:\n",
    "    count_duct[str(m)] = count_duct[str(m)] + 1\n",
    "\n",
    "# Sort the dictionary items by count in descending order\n",
    "count_duct = sorted(count_duct.items(), key=lambda d: d[1], reverse=True)\n",
    "\n",
    "# Print the unique values in the column, sorted by frequency\n",
    "print(np.array(count_duct)[:, 0])\n",
    "\n",
    "# Visualize the distribution of the column using a count plot\n",
    "# This helps to quickly see the frequency of each category\n",
    "sns.countplot(conbined_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create boolean features for common categories in several columns.\n",
    "# Explanation:\n",
    "# - For each feature, create a new column that is 1 if the condition is met, 0 otherwise.\n",
    "# - This helps simplify categorical variables and highlight important distinctions for modeling.\n",
    "\n",
    "# LotShape: Distinguish regular (\"Reg\") from irregular lot shapes.\n",
    "conbined_data[\"IsRegularLotShape\"] = (conbined_data[\"LotShape\"] == \"Reg\") * 1\n",
    "\n",
    "# LandContour: Distinguish level (\"Lvl\") land contour from others.\n",
    "conbined_data[\"IsLandContourLvl\"] = (conbined_data[\"LandContour\"] == \"Lvl\") * 1\n",
    "\n",
    "# LotConfig: Distinguish \"Inside\" lot configuration from others.\n",
    "conbined_data[\"IsLotConfigInside\"] = (conbined_data[\"LotConfig\"] == \"Inside\") * 1\n",
    "\n",
    "# LandSlope: Distinguish gentle (\"Gtl\") slope from others.\n",
    "conbined_data[\"IsLandSlopeGentle\"] = (conbined_data[\"LandSlope\"] == \"Gtl\") * 1\n",
    "\n",
    "# Condition1: Distinguish normal (\"Norm\") condition from others.\n",
    "conbined_data[\"IsCondition1Norm\"] = (conbined_data[\"Condition1\"] == \"Norm\") * 1\n",
    "\n",
    "# Condition2: Distinguish normal (\"Norm\") condition from others.\n",
    "conbined_data[\"IsCondition2Norm\"] = (conbined_data[\"Condition2\"] == \"Norm\") * 1\n",
    "\n",
    "# BldgType: Distinguish single-family (\"1Fam\") building type from others.\n",
    "conbined_data[\"IsBldgType1Fam\"] = (conbined_data[\"BldgType\"] == \"1Fam\") * 1\n",
    "\n",
    "# RoofStyle: Distinguish gable (\"Gable\") roof style from others.\n",
    "conbined_data[\"IsRoofStyleGable\"] = (conbined_data[\"RoofStyle\"] == \"Gable\") * 1\n",
    "\n",
    "# RoofMatl: Distinguish composition shingle (\"CompShg\") roof material from others.\n",
    "conbined_data[\"IsRoofMatlCompShg\"] = (conbined_data[\"RoofMatl\"] == \"CompShg\") * 1\n",
    "\n",
    "# Heating: Distinguish gas forced air (\"GasA\") heating from others.\n",
    "conbined_data[\"IsGasAHeating\"] = (conbined_data[\"Heating\"] == \"GasA\") * 1\n",
    "\n",
    "# GarageFinish: Distinguish finished (\"Fin\") garage from others.\n",
    "conbined_data[\"IsGarageFinished\"] = (conbined_data[\"GarageFinish\"] == \"Fin\") * 1\n",
    "\n",
    "# PavedDrive: Distinguish paved drive (\"Y\") from others.\n",
    "conbined_data[\"IsPavedDrive\"] = (conbined_data[\"PavedDrive\"] == \"Y\") * 1\n",
    "\n",
    "# SaleType: Distinguish warranty deed (\"WD\") sale type from others.\n",
    "conbined_data[\"IsSaleTypeWD\"] = (conbined_data[\"SaleType\"] == \"WD\") * 1\n",
    "\n",
    "# SaleCondition: Distinguish normal (\"Normal\") sale condition from others.\n",
    "conbined_data[\"IsSaleConditionNormal\"] = (conbined_data[\"SaleCondition\"] == \"Normal\") * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The only interesting \"misc. feature\" is the presence of a shed.\n",
    "# Explanation:\n",
    "# - conbined_data[\"MiscFeature\"] contains miscellaneous features for each house.\n",
    "# - This line creates a new boolean feature \"HasShed\" that is 1 if MiscFeature is \"Shed\", otherwise 0.\n",
    "# - Useful for modeling, as sheds may add value or utility to a property.\n",
    "conbined_data[\"HasShed\"] = (conbined_data[\"MiscFeature\"] == \"Shed\") * 1.  \n",
    "\n",
    "# Was this house sold in the year it was built?\n",
    "# Explanation:\n",
    "# - Checks if the house was sold in the same year it was built.\n",
    "# - \"IsVeryNewHouse\" is 1 if YearBuilt equals YrSold, otherwise 0.\n",
    "# - Indicates properties that were sold as new constructions.\n",
    "conbined_data[\"IsVeryNewHouse\"] = (conbined_data[\"YearBuilt\"] == conbined_data[\"YrSold\"]) * 1\n",
    "\n",
    "# Create boolean features for the presence/absence of certain house attributes.\n",
    "# Explanation:\n",
    "# - Each feature below is 1 if the corresponding area is zero (i.e., the house does NOT have that feature), otherwise 0.\n",
    "# - These features help the model distinguish houses with/without these amenities.\n",
    "\n",
    "conbined_data[\"Has2ndFloor\"] = (conbined_data[\"2ndFlrSF\"] == 0) * 1      # No second floor\n",
    "conbined_data[\"HasMasVnr\"] = (conbined_data[\"MasVnrArea\"] == 0) * 1      # No masonry veneer\n",
    "conbined_data[\"HasWoodDeck\"] = (conbined_data[\"WoodDeckSF\"] == 0) * 1    # No wood deck\n",
    "conbined_data[\"HasOpenPorch\"] = (conbined_data[\"OpenPorchSF\"] == 0) * 1  # No open porch\n",
    "conbined_data[\"HasEnclosedPorch\"] = (conbined_data[\"EnclosedPorch\"] == 0) * 1  # No enclosed porch\n",
    "conbined_data[\"Has3SsnPorch\"] = (conbined_data[\"3SsnPorch\"] == 0) * 1    # No 3-season porch\n",
    "conbined_data[\"HasScreenPorch\"] = (conbined_data[\"ScreenPorch\"] == 0) * 1  # No screened porch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2.Simplifications of existing features\n",
    "\n",
    "# Simplify the 'OverallQual' (overall material and finish quality) feature into three categories:\n",
    "# - 1: bad (original values 1, 2, 3)\n",
    "# - 2: average (original values 4, 5, 6)\n",
    "# - 3: good (original values 7, 8, 9, 10)\n",
    "# This reduces the granularity of the feature, making it easier for models to capture broad quality distinctions.\n",
    "conbined_data[\"SimplOverallQual\"] = conbined_data.OverallQual.replace(\n",
    "    {1 : 1, 2 : 1, 3 : 1,      # bad\n",
    "     4 : 2, 5 : 2, 6 : 2,      # average\n",
    "     7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n",
    "    })\n",
    "\n",
    "# Simplify the 'OverallCond' (overall condition rating) feature into three categories:\n",
    "# - 1: bad (original values 1, 2, 3)\n",
    "# - 2: average (original values 4, 5, 6)\n",
    "# - 3: good (original values 7, 8, 9, 10)\n",
    "# This helps to reduce noise and focus on major condition differences for modeling.\n",
    "conbined_data[\"SimplOverallCond\"] = conbined_data.OverallCond.replace(\n",
    "    {1 : 1, 2 : 1, 3 : 1,      # bad\n",
    "     4 : 2, 5 : 2, 6 : 2,      # average\n",
    "     7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3.Combinations of existing features\n",
    "\n",
    "# Overall quality of the house\n",
    "# - Multiplies 'OverallQual' (overall material/finish quality) by 'OverallCond' (overall condition rating).\n",
    "# - Creates a composite score capturing both quality and condition.\n",
    "conbined_data[\"OverallGrade\"] = conbined_data[\"OverallQual\"] * conbined_data[\"OverallCond\"]\n",
    "\n",
    "# Overall quality of the garage\n",
    "# - Multiplies encoded garage quality ('GarageQual_') by garage condition ('GarageCond').\n",
    "# - Highlights garages that are both high quality and well maintained.\n",
    "conbined_data[\"GarageGrade\"] = conbined_data[\"GarageQual_\"] * conbined_data[\"GarageCond\"]\n",
    "\n",
    "# Overall quality of the exterior\n",
    "# - Multiplies encoded exterior quality ('ExterQual_') by exterior condition ('ExterCond').\n",
    "# - Captures the combined effect of exterior materials and their condition.\n",
    "conbined_data[\"ExterGrade\"] = conbined_data[\"ExterQual_\"] * conbined_data[\"ExterCond\"]\n",
    "\n",
    "# Overall kitchen score\n",
    "# - Multiplies number of kitchens above ground ('KitchenAbvGr') by encoded kitchen quality ('KitchenQual_').\n",
    "# - Useful for distinguishing homes with multiple kitchens or higher kitchen quality.\n",
    "conbined_data[\"KitchenScore\"] = conbined_data[\"KitchenAbvGr\"] * conbined_data[\"KitchenQual_\"]\n",
    "\n",
    "# Overall fireplace score\n",
    "# - Multiplies number of fireplaces ('Fireplaces') by encoded fireplace quality ('FireplaceQu_').\n",
    "# - Highlights homes with more and/or better fireplaces.\n",
    "conbined_data[\"FireplaceScore\"] = conbined_data[\"Fireplaces\"] * conbined_data[\"FireplaceQu_\"]\n",
    "\n",
    "# Overall garage score\n",
    "# - Multiplies garage area ('GarageArea') by encoded garage quality ('GarageQual_').\n",
    "# - Captures both the size and quality of the garage.\n",
    "conbined_data[\"GarageScore\"] = conbined_data[\"GarageArea\"] * conbined_data[\"GarageQual_\"]\n",
    "\n",
    "# Overall pool score\n",
    "# - Multiplies pool area ('PoolArea') by encoded pool quality ('PoolQC_').\n",
    "# - Useful for distinguishing homes with larger and/or higher quality pools.\n",
    "conbined_data[\"PoolScore\"] = conbined_data[\"PoolArea\"] * conbined_data[\"PoolQC_\"]\n",
    "\n",
    "# Total number of bathrooms\n",
    "# - Sums basement full baths, half baths (weighted by 0.5), full baths, and half baths.\n",
    "# - Provides a single feature representing total bathroom count.\n",
    "conbined_data[\"TotalBath\"] = conbined_data[\"BsmtFullBath\"] + (0.5 * conbined_data[\"BsmtHalfBath\"]) + \\\n",
    "conbined_data[\"FullBath\"] + (0.5 * conbined_data[\"HalfBath\"])\n",
    "\n",
    "# Total yard area in square feet\n",
    "# - Sums open porch, enclosed porch, 3-season porch, and screen porch areas.\n",
    "# - Captures total outdoor/porch space.\n",
    "conbined_data[\"TotalPorchSF\"] = conbined_data[\"OpenPorchSF\"] + conbined_data[\"EnclosedPorch\"] +\\\n",
    "                                conbined_data[\"3SsnPorch\"] + conbined_data[\"ScreenPorch\"]\n",
    "\n",
    "# Total SF for house (living, basement, porch, pool)\n",
    "# - Sums living area, basement area, porch area, wood deck, and pool area.\n",
    "# - Provides a comprehensive measure of total usable space.\n",
    "conbined_data[\"AllSF\"] = conbined_data[\"GrLivArea\"] + conbined_data[\"TotalBsmtSF\"] + \\\n",
    "                         conbined_data[\"TotalPorchSF\"] + conbined_data[\"WoodDeckSF\"] + \\\n",
    "                         conbined_data[\"PoolArea\"]\n",
    "\n",
    "# House completed before sale or not\n",
    "# - Encodes whether the house was bought off-plan ('Partial' SaleCondition = 1, others = 0).\n",
    "# - Useful for identifying new builds sold before completion.\n",
    "conbined_data[\"BoughtOffPlan\"] = conbined_data.SaleCondition.replace(\n",
    "    {\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4.Polynomials on the top n existing features\n",
    "# Explanation:\n",
    "# - This section is intended for generating polynomial features (e.g., squares, cubes, interactions) from the top n features.\n",
    "# - Polynomial features can help capture non-linear relationships in the data and improve model performance.\n",
    "# - The code below prepares the training data for this purpose.\n",
    "\n",
    "train_data_new = conbined_data.iloc[:train_length,:]  # Select the first train_length rows from conbined_data as the new training set.\n",
    "# Add SalePrice column\n",
    "train_data_new['SalePrice'] = train_data['SalePrice']  # Assign the SalePrice from train_data to train_data_new for supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üì¶‚ú® Final Data Preparation\n",
    "\n",
    "This section is essential to ensure the dataset is fully prepared for modeling.  \n",
    "- üßπ **Removes unnecessary features:** Eliminates redundant columns to reduce noise and improve model performance.  \n",
    "- ü§ñ **Prepares data for machine learning:** Handles encoding, scaling, and organizes the final train/test splits.  \n",
    "- üîÑ **Maintains consistency:** Ensures both training and test sets share the same feature structure for reliable predictions.  \n",
    "- üìö **Supports reproducibility:** Documents the workflow so future analyses can be easily replicated and understood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns from train_data_new and drop the 'Id' column.\n",
    "# Explanation:\n",
    "# - train_data_new.iloc[:train_length] selects the first train_length rows (the training set).\n",
    "# - .select_dtypes(exclude=['object']) keeps only columns with numeric types (int, float).\n",
    "# - .drop(['Id'], axis=1) removes the 'Id' column, which is just an identifier and not useful for correlation analysis.\n",
    "data = train_data_new.iloc[:train_length].select_dtypes(exclude=['object']).drop(['Id'], axis=1)\n",
    "\n",
    "# Create the correlation matrix for all numeric features.\n",
    "# Explanation:\n",
    "# - .corr() computes pairwise Pearson correlation coefficients between all numeric columns.\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Select the top 20 features most correlated with 'SalePrice'.\n",
    "# Explanation:\n",
    "# - correlation_matrix['SalePrice'].abs() gets the absolute correlation values with 'SalePrice'.\n",
    "# - .nlargest(21) selects the top 21 (including 'SalePrice' itself).\n",
    "# - .index gets the names of these features.\n",
    "top_20_features = correlation_matrix['SalePrice'].abs().nlargest(21).index\n",
    "\n",
    "# Plot the correlation matrix for the top 20 correlated features.\n",
    "# Explanation:\n",
    "# - plt.figure(figsize=(12, 8)) sets the plot size.\n",
    "# - sns.heatmap() visualizes the correlation matrix for the selected features.\n",
    "# - annot=True shows the correlation values in each cell.\n",
    "# - cmap='coolwarm' sets the color map.\n",
    "# - fmt=\".2f\" formats the numbers to 2 decimal places.\n",
    "# - plt.title() adds a title to the plot.\n",
    "# - plt.show() displays the plot.\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix.loc[top_20_features, top_20_features], annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Top 20 Correlated Features with SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns in top_20_features from the data DataFrame.\n",
    "# Explanation:\n",
    "# - data[top_20_features] filters the DataFrame to keep only the columns listed in top_20_features.\n",
    "# - top_20_features contains the names of the 20 features most correlated with SalePrice, plus SalePrice itself.\n",
    "# - This step is useful for focusing analysis or modeling on the most important features, reducing dimensionality and potential noise.\n",
    "data = data[top_20_features]\n",
    "\n",
    "# Print the names of the columns in the filtered data DataFrame.\n",
    "# Explanation:\n",
    "# - print(data.columns) displays the column names after filtering, allowing you to verify which features are included.\n",
    "# - This is helpful for debugging and confirming that only the desired columns are present.\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules for modeling.\n",
    "# from xgboost import XGBRegressor  # (commented out, not used here)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare target and features for training.\n",
    "# - train_Y: The target variable (SalePrice) from the filtered data.\n",
    "# - data.drop(['SalePrice'], axis=1, inplace=True): Remove SalePrice from features to avoid data leakage.\n",
    "train_Y = data['SalePrice']\n",
    "train_Id = conbined_data.iloc[:train_length, 0]\n",
    "test_Id = conbined_data.iloc[train_length:, 0]\n",
    "\n",
    "data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Split the data into training and validation sets.\n",
    "# - train_test_split: Randomly splits the data into train and test sets (80% train, 20% test).\n",
    "# - random_state=42 ensures reproducibility.\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data, train_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and fit a linear regression model.\n",
    "# - regressor: LinearRegression model object.\n",
    "# - regressor.fit(train_X, train_Y): Train the model using training features and target.\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(train_X, train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error from scikit-learn to evaluate regression performance.\n",
    "# Explanation:\n",
    "# - mean_squared_error computes the average squared difference between predicted and actual values.\n",
    "# - It is a common metric for regression tasks, indicating how close predictions are to true values.\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "# Use the trained regressor to predict SalePrice values for the test set.\n",
    "# - test_X: Features for the validation/test split.\n",
    "# - regressor.predict(test_X): Generates predicted SalePrice values.\n",
    "test_Y_pred = regressor.predict(test_X)\n",
    "\n",
    "# Calculate the mean squared error between actual and predicted SalePrice values.\n",
    "# - mean_squared_error(test_Y, test_Y_pred): Compares true SalePrice (test_Y) to predictions.\n",
    "# - Lower MSE indicates better model performance.\n",
    "mse = mean_squared_error(test_Y, test_Y_pred)\n",
    "mae = mean_absolute_error(test_Y, test_Y_pred)\n",
    "rmse = root_mean_squared_error(test_Y, test_Y_pred)\n",
    "# Print the mean squared error to assess model accuracy.\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot predicted vs actual SalePrice values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(test_Y, test_Y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual SalePrice\")\n",
    "plt.ylabel(\"Predicted SalePrice\")\n",
    "plt.title(\"Predicted vs Actual SalePrice\")\n",
    "plt.plot([test_Y.min(), test_Y.max()], [test_Y.min(), test_Y.max()], color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Save the trained regression model to disk using pickle.\n",
    "# Explanation:\n",
    "# - model_path specifies the directory where the model file will be saved (\"models\").\n",
    "# - If the directory does not exist, os.makedirs(model_path) creates it.\n",
    "# - The model is saved as 'linear_regression_model.pkl' inside the models directory.\n",
    "# - pickle.dump(regressor, file) serializes the trained regressor object and writes it to the file.\n",
    "# - This allows you to reload and use the trained model later without retraining.\n",
    "\n",
    "model_path=\"models\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "with open(os.path.join(model_path, 'linear_regression_model.pkl'), 'wb') as file:\n",
    "    pickle.dump(regressor, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the needed features of the model to make prediction.\n",
    "# Explanation:\n",
    "# - regressor.feature_names_in_ contains the names of the features used to train the regression model.\n",
    "# - The for loop iterates over each feature name.\n",
    "# - train_X[feature].dtype retrieves the data type of each feature in the training set.\n",
    "# - The print statement displays the feature name and its data type.\n",
    "# - This is useful for verifying which features are required for prediction and ensuring the input data matches the expected types.\n",
    "for feature in regressor.feature_names_in_:\n",
    "    print(f\"Feature: {feature}, type:{train_X[feature].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost==3.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "# Instantiate and fit an XGBoost regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=200, max_depth=8, random_state=42)\n",
    "xgb_regressor.fit(train_X, train_Y)\n",
    "\n",
    "# Predict and evaluate\n",
    "test_Y_pred = xgb_regressor.predict(test_X)\n",
    "mae = mean_absolute_error(test_Y, test_Y_pred)\n",
    "mse = mean_squared_error(test_Y, test_Y_pred)\n",
    "rmse = root_mean_squared_error(test_Y, test_Y_pred)\n",
    "print(f\"XGBoost Mean Squared Error: {mse}\")\n",
    "print(f\"XGBoost Mean Absolute Error: {mae}\")\n",
    "print(f\"XGBoost Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict SalePrice for the test set using the trained XGBoost regressor\n",
    "test_Y_pred = xgb_regressor.predict(test_X)\n",
    "\n",
    "# Plot predicted vs actual SalePrice values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(test_Y, test_Y_pred, alpha=0.6)\n",
    "plt.xlabel(\"Actual SalePrice\")\n",
    "plt.ylabel(\"Predicted SalePrice\")\n",
    "plt.title(\"Predicted vs Actual SalePrice (XGBoost)\")\n",
    "plt.plot([test_Y.min(), test_Y.max()], [test_Y.min(), test_Y.max()], 'r--')  # Diagonal line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Test and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows from train_X and test_X, reset index for clean DataFrames\n",
    "save_train = train_X.copy().drop_duplicates().reset_index(drop=True)\n",
    "save_test = test_X.copy().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Add 'Id' column to save_train and save_test using train_Id and test_Id\n",
    "# - Ensures each row has a unique identifier for later reference or submission\n",
    "save_train['Id'] = train_Id.values[:len(save_train)]\n",
    "save_train['SalePrice'] = train_Y.values[:len(save_train)]  # Add target variable to train set\n",
    "save_test['Id'] = test_Id.values[:len(save_test)]           # Only Id for test set (no SalePrice)\n",
    "\n",
    "# Save processed train and test sets to CSV files for future use or submission\n",
    "save_train.to_csv(\"data/new_train.csv\", index=False)\n",
    "save_test.to_csv(\"data/new_test.csv\", index=False)\n",
    "\n",
    "# Load the saved test set from CSV for prediction or inspection\n",
    "test_df = pd.read_csv(os.path.join('data', 'new_test.csv'))\n",
    "test_df.head()  # Display first few rows for verification\n",
    "\n",
    "# Remove 'SalePrice' from top_20_features (if present) to get only feature columns\n",
    "top_20_features = top_20_features.drop('SalePrice')\n",
    "\n",
    "# Select only the top 20 features from test_df for final prediction input\n",
    "test_final = test_df[top_20_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_predictions = xgb_regressor.predict(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission = pd.DataFrame({'Id': test_df['Id'], 'SalePrice': kaggle_predictions})\n",
    "\n",
    "kaggle_submission.to_csv(os.path.join('data', 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c house-prices-advanced-regression-techniques -f data/submission.csv -m \"Message\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "kaggle-house-prices",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
